### bagging
##### 基本概念
bagging方法bootstrap aggregating的缩写，采用的是随机有放回的选择训练数据然后构造分类器，最后组合。
这里以随机森林为例进行讲解。

##### 主要思想/基本步骤
随机森林算法是上世纪八十年代Breiman等人提出来的，其基本思想就是构造很多棵决策树，形成一个森林，然后用这些决策树共同决策输出类别是什么。随机森林算法及在构建单一决策树的基础上的，同时是单一决策树算法的延伸和改进。在整个随机森林算法的过程中，有两个随机过程，第一个就是输入数据是随机的从整体的训练数据中选取一部分作为一棵决策树的构建，而且是有放回的选取；第二个就是每棵决策树的构建所需的特征是从整体的特征集随机的选取的，这两个随机过程使得随机森林很大程度上避免了过拟合现象的出现。
随机森林算法具体的过程：
1. 从训练数据中选取n个数据作为训练数据输入，一般情况下n是远小于整体的训练数据N的，这样就会造成有一部分数据是无法被去掉的，这部分数据成为袋外数据，可以使用袋外数据做误差估计。
2. 选取了输入的训练数据之后，需要构建决策树，具体方法是每一个分裂结点从整体的特征集M中选取m个特征构造，一般情况下m远小于M。
3. 在构造每颗决策树的过程中，按照选取最小的基尼指数进行分裂节点的选取进行决策树的构建。
4. 重复第2步和第3步多次，每一次训练n个数据对应一颗决策树。

##### 适用场景

##### 优缺点
* 优点:
> 不容易过拟合，鲁棒性强
> 能够处理高维数据，不用做特征选择，既能处理离散数据，又能处理连续型数据，数据集无需规范化。
> 在创建随机森林的时候，对generalization error使用的是无偏估计。
> 训练速度快，可以得到变量重要性排序。
> 在训练过程中，能够检测到feature间的互相影响。
> 可以并行，实现比较简单。

##### 相关问题
