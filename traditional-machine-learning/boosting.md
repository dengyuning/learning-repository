### boosting(提升算法)
##### 基本概念
Boosting方法是一种用来提高弱分类算法准确度的方法，是集成学习方法的一种。

##### 主要思想/基本步骤
1. AdaBoost是最著名的boosting算法。开始时，所有样本的权重相同，训练得到第一个及分类器。从第二轮开始，每轮开始前都先根据上一轮及分类器的分类效果调整每个样本的权重（提高分错样本的权重，降低分对样本的权重）。

2. GBDT是通过采用加法模型（即基函数的线性组合），以及不断减小训练过程产生的残差来达到将数据分类或者回归的算法。
> GBDT通过多轮迭代，每轮迭代产生一个弱分类器，每个分类器在上一轮分类器的残差基础上进行训练。对弱分类器的要求一般是足够简单，而且是低方差和高偏差的。
> 弱分类器一般会选择为CART TREE(也就是分类回归树)。由于上述高偏差和简单的要求，每个分类回归树的深度不会很深。最终的总分类器是将每轮训练得到的弱分类器加权求和得到的。
> 利用损失函数的负梯度在当前模型的值作为回归问题提升树算法中的残差的近似值去拟合一个回归树。gbdt每轮迭代的时候，都会去拟合损失函数在当前模型下的负梯度。
> CART TREE(是一种二叉树)生成的过程其实就是一个特征选择的过程。假设我们目前总共有 M 个特征。第一步我们需要从中选择出一个特征 j，做为二叉树的第一个节点。然后对特征 j 的值选择一个切分点 m. 一个 样本的特征j的值 如果小于m，则分为一类，如果大于m,则分为另外一类。如此便构建了CART 树的一个节点。
> 在CTR预估中，工业界一般会采用逻辑回归去进行处理。逻辑回归本身是适合处理线性可分的数据，如果我们想让逻辑回归处理非线性的数据，其中一种方式便是组合不同特征，增强逻辑回归对非线性分布的拟合能力。
> gbdt 无论用于分类还是回归一直都是使用的CART 回归树。不会因为我们所选择的任务是分类任务就选用分类树，这里面的核心是因为gbdt 每轮的训练是在上一轮的训练的残差基础之上进行训练的。这里的残差就是当前模型的负梯度值 。这个要求每轮迭代的时候，弱分类器的输出的结果相减是有意义的。残差相减是有意义的。


##### 适用场景

##### 优缺点


##### 相关问题
